{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentimental_analysis_IMDb_2_200801.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMyLJQC0GpHfInMtRfS6sBg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonas-jun/nlp_imdb_sentiment/blob/master/sentimental_analysis_IMDb_2_200801.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tjeNFpERKgE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a1887e9-83aa-4eb5-9c86-b68a962bc6fa"
      },
      "source": [
        "!pip install torchtext\n",
        "!pip install spacy\n",
        "!python -m spacy download en"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.7.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (49.2.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (49.2.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv3RJ7WxRopL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time # calculate time for an epoch\n",
        "import spacy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu5hPuRwT6t-",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ly-YntuSSFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "TEXT = data.Field(tokenize='spacy', include_lengths=True) # include_lengths\n",
        "LABEL = data.LabelField(dtype=torch.float)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXv-TBkbRXQU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bfbb3713-734e-41b3-fbae-90f02cef0ade"
      },
      "source": [
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "print('number of training examples: {:,}'.format(len(train_data)))\n",
        "print('number of test examples: {:,}'.format(len(test_data)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 10.8MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of training examples: 25,000\n",
            "number of test examples: 25,000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7hVPLC5Rk63",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "388f3125-a206-4b68-c758-da79938fad0b"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['Otto', 'Preminger', \"'s\", 'Dana', 'Andrews', 'cycle', 'of', 'films', 'noirs', 'are', 'among', 'the', '(', 'largely', ')', 'unsung', 'jewels', 'of', 'the', 'genre', '.', 'Because', 'they', 'lack', 'paranoia', ',', 'misogyny', 'or', 'hysteria', ',', 'they', 'may', 'have', 'seemed', 'out', 'of', 'place', 'at', 'the', 'time', ',', 'but', 'the', 'clear', '-', 'eyed', 'imagery', ',', 'the', 'complex', 'play', 'with', 'identity', ',', 'masculinity', 'and', 'representation', ',', 'the', 'subversion', 'of', 'traditional', 'psychological', 'tenets', ',', 'the', 'austere', ',', 'geometrical', 'style', 'all', 'seem', 'startlingly', 'modern', 'today', ',', 'and', 'very', 'similar', 'to', 'Melville', '.', 'The', 'lucid', 'ironies', 'of', 'this', 'film', 'are', 'so', 'loaded', ',', 'brutal', 'and', 'ironic', 'that', 'the', \"'\", 'happy', \"'\", 'ending', 'is', 'one', 'of', 'the', 'cruellest', 'in', 'Hollywood', 'history', '.', 'Brilliant', 'on', 'the', 'level', 'of', 'entertaining', 'thriller', 'as', 'well', ',', 'tense', ',', 'and', 'packed', 'with', 'double', '-', 'edged', 'dialogue', '.'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtogWUbwSzZa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "53e349a6-9415-4be9-f67d-05e00ab30143"
      },
      "source": [
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED)) # default ratio is 0.7\n",
        "\n",
        "print('number of training examples: {:,}'.format(len(train_data)))\n",
        "print('number of validation examples: {:,}'.format(len(valid_data)))\n",
        "print('number of test examples: {:,}'.format(len(test_data)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training examples: 17,500\n",
            "number of validation examples: 7,500\n",
            "number of test examples: 25,000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0ggb1CQUUAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_vocab_size = 25000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size=max_vocab_size, vectors='glove.6B.100d', unk_init=torch.Tensor.normal_) # glove 100 dimension word embedding\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi9rDFkVCxrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c381838f-36bb-4bf3-9e88-aa7523f3b56f"
      },
      "source": [
        "TEXT.vocab.itos[:10]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hNsSxooVIU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iter, valid_iter, test_iter = data.BucketIterator.splits(\n",
        "    datasets=(train_data, valid_data, test_data), batch_size=BATCH_SIZE,\n",
        "    sort_within_batch=True, device=device)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsmien8nXS7D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fecf4beb-0f47-46d8-8d4d-b5c2b5cf382b"
      },
      "source": [
        "print('number of train iter: {:,}'.format(len(train_iter)))\n",
        "print('number of valid iter: {:,}'.format(len(valid_iter)))\n",
        "print('number of test iter: {:,}'.format(len(test_iter)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of train iter: 274\n",
            "number of valid iter: 118\n",
            "number of test iter: 391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-2aCkOEXhyG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1103cc4d-d881-47a4-f911-32721ef6edaf"
      },
      "source": [
        "int(len(train_data) / BATCH_SIZE) # approximately same with length of train iter"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "273"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ysjs9XkrXn_i",
        "colab_type": "text"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "- input dimension: length of one-hot vectors\n",
        "- embedding dimension: size of dense word vectors, usually around 50-250 dimensions\n",
        "- hidden dimension: size of the hidden states\n",
        "- output dimension: number of classes, in this case 1, because only 2 cases, 0 or 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_qvluUdYOfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim) # bidirectional model이기 때문에?\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "        embedded = self.dropout(self.embedding(text)) # word를 embedding 후 1차 dropout을 거침\n",
        "\n",
        "        # pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "\n",
        "        # unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)) # hidden_dim 길이의 2가지 output을 concatenate\n",
        "\n",
        "        return self.fc(hidden) # 2*hidden_dim, output_dim(0 or 1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVBBywhRZwEZ",
        "colab_type": "text"
      },
      "source": [
        "Like before, we'll create an instance of our RNN class, with the new parameters and arguments for the number of layers, bidirectionality and dropout probability.\n",
        "\n",
        "To ensure the pre-trained vectors can be loaded into the model, the EMBEDDING_DIM must be equal to that of the pre-trained GloVe vectors loaded earlier.\n",
        "\n",
        "We get our pad token index from the vocabulary, getting the actual string representing the pad token from the field's pad_token attribute, which is 'pad' by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBCS6m2waE9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = len(TEXT.vocab)\n",
        "embedding_dim = 100 # glove.100d\n",
        "hidden_dim = 256\n",
        "output_dim = 1\n",
        "n_layers = 2\n",
        "bidirectional = True # bidirectional을 True로 주면 num_layers는 자연스레 2개가 되는 거 아닌가?\n",
        "dropout = 0.5\n",
        "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = RNN(vocab_size=input_dim,\n",
        "            embedding_dim=embedding_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            output_dim=output_dim,\n",
        "            n_layers=n_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout,\n",
        "            pad_idx=pad_idx)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWaR2_zdbH0V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1dbbcda6-fc7a-438d-fec8-8103318b8ab2"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('The model has {:,} trainable parameters'.format(count_parameters(model)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 4,810,857 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyKOi4EHb5X1",
        "colab_type": "text"
      },
      "source": [
        "The final addition is copying the pre-trained word embeddings we loaded earlier into the embedding layer of out model.\n",
        "\n",
        "We retrieve the embeddings from the field's vocab, and check they're the correct size, [vocab size, embedding dim]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrv2txk_cL_3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "63ecaf26-1451-47cd-d11a-8419eea052a2"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxO7f1KccMIz",
        "colab_type": "text"
      },
      "source": [
        "Then, we replace the initial weights of the embedding layer with the pre-trained embeddings.\n",
        "\n",
        "NOTE: this should always be done on the weight.data and not the weight!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdG5Tko8NK_0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96b679e7-b8fd-4d41-f120-7e314583b15f"
      },
      "source": [
        "len(pretrained_embeddings[1]) # 1개 단어 당 length 100의 vector로 이뤄짐"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJMeyT3echeS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "fb77e9d1-7ac8-404f-8eb7-8c975c8f6890"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9597,  0.8905, -0.7076,  ...,  0.3940, -1.2075, -0.9683],\n",
              "        [-0.3404,  0.2269,  0.0731,  ..., -0.4427,  0.6267,  0.2811],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 0.4765,  0.2254,  0.3035,  ..., -0.2082,  0.1948,  0.8972],\n",
              "        [-0.1983, -0.2634,  0.4227,  ...,  0.1574, -0.3458,  0.4537],\n",
              "        [ 0.3203, -0.8293,  1.2617,  ..., -0.8267,  1.5076, -0.0893]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZnvvfP3clNz",
        "colab_type": "text"
      },
      "source": [
        "As our 'unk' and 'pad' token aren't in the pre-trained vocabulary they have been initialized using unk_init (N(0,1) distribution) when building our vocab. it is preferable to initialize them both to all zeros to explicitly tell our model that, initially, they are irrelevant for determining sentiment.\n",
        "\n",
        "We do this by manually setting their row in the embedding weights matrix to zeros. We get their row by finding the index of the tokens, which we have already done for the padding index.\n",
        "\n",
        "NOTE: like initializing the mebeddings, this should be done one the weight.data and not the weight!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWzl9AP4dGq-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "1dd3da3a-52f4-44fc-88ab-5d68ea0a1002"
      },
      "source": [
        "unk_idx = TEXT.vocab.stoi[TEXT.unk_token] # pad idx는 위에서 할당\n",
        "\n",
        "model.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
        "\n",
        "print(unk_idx, pad_idx)\n",
        "print(model.embedding.weight.data.shape)\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1\n",
            "torch.Size([25002, 100])\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [ 0.4765,  0.2254,  0.3035,  ..., -0.2082,  0.1948,  0.8972],\n",
            "        [-0.1983, -0.2634,  0.4227,  ...,  0.1574, -0.3458,  0.4537],\n",
            "        [ 0.3203, -0.8293,  1.2617,  ..., -0.8267,  1.5076, -0.0893]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqeT5sb2eDLf",
        "colab_type": "text"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "    argument 1: parameters we'll update\n",
        "    argument 2: learning rate\n",
        "\n",
        "    > change optimization method SGD to Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3qo9uTZe3NX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device) # which things are should read to cuda?"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OErjTB2Ne3dJ",
        "colab_type": "text"
      },
      "source": [
        "#### Accuracy Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaG9zkU9feY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    '''\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8 NOT 8\n",
        "    '''\n",
        "\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() # convert to float for division\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yLMz8tLge_C",
        "colab_type": "text"
      },
      "source": [
        "#### Definitions for training and evaluating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr_HT2kffzPM",
        "colab_type": "text"
      },
      "source": [
        "As we have set include_lengths = True, our batch.text is now a tuple with the first element being the numericalized tensor and the second element being the actual lengths of each sequence. We seperate these into their own variables, text and text_lengths, before passing them to the model.\n",
        "\n",
        "NOTE: as we are now using dropout, we must remember to use model.train() to ensure the dropout is 'turrned on' while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnCRgw3mgU7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        text, text_lengths = batch.text\n",
        "\n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item() # .item() ?\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxBjS6p4hb0U",
        "colab_type": "text"
      },
      "source": [
        "As we are now using dropout, we must remember to use model.eval() to ensure the dropout is 'turned off' while evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeTS9LD0hmcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# do not need to optimize, do back propagation\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, text_lengths = batch.text\n",
        "\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxnp_2oYiQXB",
        "colab_type": "text"
      },
      "source": [
        "Create function to tell us how long an apoch takes to compare training times between models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAc3bpzPiVQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins*60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRYKO7tIiVVo",
        "colab_type": "text"
      },
      "source": [
        "#### Train the model multiple epochs\n",
        "\n",
        "At each epoch, if the validation loss is the best we have seen so far, we'll save the parameters of the model and then after training has finished we'll use that model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgHqR2jIjHmE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "8517ee07-7224-4448-a02a-e67c577b808d"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "\n",
        "    print('Epoch: {:02} | Epoch Time: {}m {}s'.format(epoch+1, epoch_mins, epoch_secs))\n",
        "    print(f'\\tTrain Loss: {train_loss:0.03f}, | Train_acc: {(train_acc*100):0.02f}')\n",
        "    print(f'\\tValid Loss: {valid_loss:0.03f}, | Valid_acc: {(valid_acc*100):0.02f}') "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 1m 40s\n",
            "\tTrain Loss: 0.645, | Train_acc: 61.49\n",
            "\tValid Loss: 0.540, | Valid_acc: 74.33\n",
            "Epoch: 02 | Epoch Time: 1m 39s\n",
            "\tTrain Loss: 0.541, | Train_acc: 72.68\n",
            "\tValid Loss: 0.464, | Valid_acc: 78.41\n",
            "Epoch: 03 | Epoch Time: 1m 39s\n",
            "\tTrain Loss: 0.442, | Train_acc: 79.96\n",
            "\tValid Loss: 0.389, | Valid_acc: 82.91\n",
            "Epoch: 04 | Epoch Time: 1m 39s\n",
            "\tTrain Loss: 0.362, | Train_acc: 84.57\n",
            "\tValid Loss: 0.348, | Valid_acc: 85.27\n",
            "Epoch: 05 | Epoch Time: 1m 39s\n",
            "\tTrain Loss: 0.321, | Train_acc: 86.64\n",
            "\tValid Loss: 0.329, | Valid_acc: 86.02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmbDFyhSkPgk",
        "colab_type": "text"
      },
      "source": [
        "## Apply Testset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMv4dKjWELRc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e5bd7f5-ebee-4b5a-d75c-f29bb18837db"
      },
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "test_loss, test_acc = evaluate(model, test_iter, criterion)\n",
        "print('Test Loss: {:0.03f} | Test Acc: {:0.02f}%'.format(test_loss, test_acc*100))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.340 | Test Acc: 85.40%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMcb_IquEmMd",
        "colab_type": "text"
      },
      "source": [
        "## Test with User Input\n",
        "\n",
        "We can now use our model to predict the sentiment of any sentence we give it. As it has benn trained on movie reviews, the sentences provided should also be movie reviews.\n",
        "\n",
        "When using a model for inference it should always be in evaluation mode. If this tutorial is followed step-by-step then it should already be in evaluation mode (from doing evaluate on the test set), however we explicitly set it to avoid any risk\n",
        "\n",
        "Our predict_sentiment function does a few things:\n",
        "\n",
        "1. sets the model to evaluation mode\n",
        "2. tokenizes the sentence, i.e. splits it from a raw string into a list of tokens\n",
        "3. indexes the tokens by converting them into their integer representation from our vocabulary\n",
        "4. gets the length of our sequence\n",
        "5. converts the indexes, which are a Python list into a PyTorch tensor\n",
        "6. add a batch dimension by unsqueezeing\n",
        "7. converts the length into a tensor\n",
        "8. squashes the output prediction from a real number between 0 and 1 with the sigmoid function\n",
        "9. converts the tensor holding a single value into an integer with the item() method\n",
        "\n",
        "We are expecting reviews with a negative sentiment to return a value close to 0 and positive reviews to return a value close to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHTAiGQxFXLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "\n",
        "    return prediction.item()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxTVABBqGDT5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4618c2a6-c902-4b6e-a2a8-db125efc1285"
      },
      "source": [
        "predict_sentiment(model, \"It was really boring, and I felt that I would like to stop this travel and quit the theater.\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03796844184398651"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfkMk0PSGTAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}